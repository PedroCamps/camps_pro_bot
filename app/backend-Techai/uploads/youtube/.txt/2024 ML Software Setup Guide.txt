 Hey what's up guys! Today we're gonna be building and setting up a machine learning server that I can use to offload machine learning jobs for my main gaming computer, leaving it free to play video games or edit videos. We're going to be using these parts and once the computer is built, we'll install Linux instead of all of my favorite pieces and software I use for my daily ML projects. The goal is for me to be able to remote into this computer and launch a training job, speak to a local LLM, or run some stable diffusion. Let's jump right into the parts that we have for today. Given that this machine will be used solely for machine learning, let's start off with the most important part. The Nvidia GeForce RTX 4080 Super. It has 16GB of GDDR6 memory and over 10,000 kt. I can't wait to make those cores crunch some matrix multiplications. While originally designed for video games, the ML community soon figured out that the highly parallelizable tasks, something graphics cards excel at, in ML, map really well to the graphics hardware. 16GB of VRAM should be enough to run LLMs like LAMA or Mixtra locally, albeit they'll probably have to be quantized. We'll talk more about LLMs later this video. The brains of this machine will be the Intel 14900K, having a good CPU is also very important for ML tasks, because it's what will be handling the less parallelizable tasks, such as data preprocessing, and moving that data between RAM and VRAM. Also, some ML algorithms that aren't heavy in matrix multiplications are more sequential in nature, benefiting from a more powerful CPU. Speaking of RAM, we got 96GB of GDDR5 RAM from G-Skill. After the 16GB of VRAM gets saturated from large data sets or models, the system will rely on this RAM, as it's the next tier of the caching hierarchy, so it's good to have plenty just in case. We should avoid going to the disk at all costs. For persistent storage, I'll be using this 1TB Samsung 980 Pro NVMe SSD that I'd line around. We'll store all of the model weights and data that aren't in flight, in here, so hopefully 1TB will serve our purposes. The glue of the system, the motherboard, will be the ASUS ROG Strix Z790i Gaming motherboard. It has the newest Intel chipset, for the LGA 1700 socket, so it should allow our components to perform at their peaks. For the CPU cooler, I have the NZXT Kraken 280 AIO cooler. It should be enough to keep our CPU temps cool while the CPU is doing our bidding. It also comes with this display to show PC metrics like temperature or component utilization. For the power supply, I have the 1000W SFX L power supply from Corsair. The nice thing is that this supports the new 12V high power connector, but the new 40 series cards have due to their ever increasing power demands. It also allows for more direct communication between the GPU and the power supply. And last, but not least, the case that will be holding all of our components as cooler masters new NR200 PV2 case. It should be able to provide our components with adequate airflow. We'll be putting in a Corsair IF-120 slim fan at the bottom of it to accompany the CPU cooler's fans. So, let's get to finally building this PC. Let's begin with installing the components that go on our motherboard. We start with the CPU. I slowly lower it into the socket and lock it in place. Then, we carefully insert the RAM dims in their slots. Next, we prepare our CPU cooler. We screw in the cooler bracket on the board and then mount the fans on the rad. Since the radiator will be at the top of the case, it'll be acting as an exhaust. Next, we bring out the case. This is my first time working in a small form factor case and my god was the challenging. All of the parts barely fit and I was surprised I didn't break anything. To access the inside, I had to remove all of the side panels and unscrew some of the railings. We slowly placed the motherboard in the case and screwed it onto the pre-installed standoffs. Then, because I forgot to do this earlier, I insert the NVMe drive into the motherboard. Probably would have been easier if I had done this at the beginning, but oh well. Next, we prepare our power supply. Because of the case's form factor, the PSU goes on the side rather than the bottom of the case. That means the case has to come with an extension cable to power the PSU. Now, we get to mounting the radiator at the top of the case. It was a really tight fit with only a couple millimeters of clearance and I had to reorientate the fans to be pulling through the radiator and out the case. Next, we do some cable management and add the PSU cables to power our components. We also install the pump block onto the CPU. Afterwards, I removed the GPU brace from the case and attached it to the 4D80 Super. And inserting the GPU was also a chore because of all of the cables getting in the way. But finally, everything fell into place. After I installed the bottom fan alongside the fan that already came with the case, setting them to be intakes. I reinstalled the panels and ta-da, the build is finally complete after several, arduous hours. Let's not switch over to the software side of things. We're going to be installing a Bluntu server 22.04 LTS, codenamed Jami Jellyfish, which is the latest long-term support version of a Bluntu. Since we'll only be remodeling into the server, I really don't want to waste any resources on the whole desktop environment, which is why I'm electing for the server version. However, if working with just the terminal seems daunting, don't worry. We'll be covering some apps that'll still allow us to interact with our server through a GUI. Not to mention that if you really want a desktop environment, everything will be installing today will work exactly the same on the desktop release of a Bluntu. Also, because this is an LTS release, canonical has guaranteed stability and security updates until April of 2027. I prefer working with the LTS release because it tends to be more stable, especially with the various pieces of software will be installing today. Those of you that have seen my NAS video know that I'm a huge fan of Proxmox, which would allow me to be able to run virtual machines on the server. While Proxmox probably seems more appropriate for a server rather than a Bluntu, this machine only has one GPU. GPU virtualization, which can split up the GPU across multiple VMs, isn't really something I want because I want the GPU to be solely dedicated to just one ML task at a time. And since I'll be the only one using it, I rather just have the OS be as close to the bare metal as possible. We'll talk about Docker though, later this video which will still let me run multiple jobs at the same time if I really want to. I downloaded the ISO from the Ubuntu website and flashed it onto a flash drive with Rufus. I then plugged in the flash drive, a keyboard, a monitor, and ethernet before I turned the machine on. As it was starting up, I jammed the delete key because that's how I boot into the BIOS for my specific motherboard. I selected this flash drive as the boot device and hit save and exit. The actual install process is pretty simple. I first noted down the IP address that was assigned to my machine because we'll need this later. Then, I chose the 1TB NVMe drive as the install disk, set up my profile, and enabled open SSH so we can remote into the server. And that's it. After the install process is complete, I press reboot. Once the machine has rebooted and the login screen has shown up, I unplug all the peripherals because we will no longer be directly working with the server. Now, from my gaming computer, I can remotely connect to the server with SSH because both machines are on my home network. I used the IP address that I noted down earlier and the password that I set up during the install. I run the classic sudo apt update and a sudo apt upgrade just to ensure that everything is up to date. If I run H top, we can see all 32 cores of our CPU and our 96GB of RAM. The first piece of software I always install whenever I have a fresh install of Linux is called tail scale. Tail scale is a VPN service that enables all of my devices to talk directly to each other as if they were on the same network. This is unnecessary if I'm only using my gaming computer, which always stays at home to access my server. But now I can remote into my server from my laptop or even my iPad from anywhere in the world, provided that they also have tail scale installed and set up. While tail scale isn't directly related to machine learning in any way, it makes managing my server remotely so much easier. I run the tail scale install script from their website, run tail scale up and authenticate with my tail scale account. Now this server is truly my personal cloud server, one that I can access from anywhere at any time. The next thing we need to install are our graphics drivers. This step took a little bit of trial and error, but the way that worked best for me was to directly download the Nvidia drivers from their website. I selected the graphics card that I had and the OS and then copied the download link to WGet in the terminal. I'll leave the exact steps I followed down below, but after the download completed I had to change some values in a couple of different files. Then I simply ran the run file and accepted all the suggested values. Not sure if my issues stem from an unlucky combination of my hardware, kernel version or driver version, but hopefully your experience is a little bit smoother. Finally, to ensure that the drivers were successfully installed, we run the Nvidia SMI command. And ta-da! We see the name of our graphics card. Now let's get some LLMs running locally. We'll be using an app called Olamma, which is probably the easiest way to get up and running. You'll be able to use Olamma even if your computer doesn't have a GPU. They have a wide variety of LLMs that you can pick and choose. Some are better for natural language, while others are better at coding tasks. After running their install script, all we need to do is run Olamma run and the name of the model you want. I chose MixedRoll. After a brief download process, we're able to chat with the LLM. We can ask it anything our heart desires. The poem being generated on screen isn't real time and hasn't been sped up at all. It's reasonably fast. When choosing what LLM to run, you need to take into account the amount of VRAM your GPU has and what the expected usage is for the specific model. Some models will list their requirements on their Olamma page. If a model is running really slowly, consider using a more heavily quantized version, which uses less precision for the model parameters, resulting in less VRAM usage. Here's a quick snapshot of our GPU utilization during token generation. It's using about 50 watts of power and our VRAM is fully saturated. Let's see how we can use a code LLM to help us program. My go to IDE whenever I'm working on my ML projects is always VS code. Because of its wide array of extensions. And you guessed it, there's one that lets a local code LLM help us out while we're programming. Now, we could use VS code on my gaming computer to SSH into our server, but let me show you a cooler way. The nice thing with VS code is that it's built with electron, meaning that it can run in our browser as a progressive web app. Code server is a service that we can install on our server in order to be able to access a web-based VS code session from any device that's on our tail scale network. Their install command is provided on their website. After running it, I use System CTL to have code server run as a background service. I also edit the values in the config.yaml file to make sure the service is listening on all interfaces so our other tail-scaled devices can access it. After restarting the service, I navigate to the IP address of my server in my web browser at the port 8080. Now, we have a fully fledged programming environment that has a GPU and can be accessed from any web browser on our tail scale network. My favorite feature is probably the fact that you can access this website from your iPad and save it to your home screen so you can access it next time as if it's just a regular app. Connect a keyboard and mouse to your iPad and you have exactly the same experience that you'd have on your laptop. The great thing is that the terminal works exactly how you'd expect, allowing you to even connect to other machines on your network if needed. Before we run a code out of the locale, let's make sure we have Python installed. I personally love to use miniconda which is a lighter distribution of anaconda. This allows us to have multiple installations of Python simultaneously on our computer, which is useful for when we need different versions of Python or different accompanying packages for different projects. To install miniconda, I run these commands which can be found under the miniconda section on the anaconda website. And if I run conda-version, we see that we're good to go. Running which Python also shows the path of the Python executable and I'm also able to print out hello world in an interactive Python show. Now let's connect VS code to our lm and generate some code. The one I'll be using today is code llama 13b. But feel free to choose a different one. We can download it with the olama run command again. Now we can use the continue VS code extension to talk to our code generation lm. After installing it, we have to update the config.json file to let it know that we'll be using code llama. I also made sure our tab auto completion feature uses code llama. Now we can chat with it directly from the IDE and also use it for code auto completion. I go through the sample tutorial that the continue extension provides. When I highlight the first function, code llama correctly identifies the algorithm to be the bubble sort algorithm. When I ask it to generate comments for the next function, it also does so while neatly identifying the input parameters and return type. Finally, I can also have it help me debug an issue. I try to add a list of strings and the extension lets me directly prompt the lm with the error in my terminal output. The lm correctly finds the issue, but its solution isn't necessarily the most robust. You can also connect your VS code installation from another computer on your network to the lm being served on your server. I had to make the lm server listen to all interfaces on my server and then I could simply point the continue extension on my gaming computer to the IP address of my server. The default port for lm is 11434. You simply have to change the API base field in the config.json file. Thanks to tailscales DNS resolution, I can just use the name of my server. Because both machines are on my home network, communication between the two is blazingly fast, letting me program without a hitch. Now, my gaming computer doesn't have to deal with the burden of running that alone locally. Let's now set up PyTorch, which we'll use in just a bit to run a local video generation model. We'll first create a new conda environment named local diffusion. PyTorch's website has a very simple section that gives us the correct install command based on our OS and whether we want the GPU version with CUDA or just the CPU version. All we have to do is simply run this command in our conda environment. I recommend you go with the latest CUDA version unless you specifically need the older version. If you're on Windows or Mac, make sure you click the respective boxes for your OS to get the right install command. And now, we're done. We can have our local adalarm generate a small script to ensure that PyTorch can see our GPU correctly. Perfect. Exactly what we wanted. The reason this process was so smooth is because we already did the heavy lifting of configuring our graphics drivers earlier. Let's go to Hugging Face to check out a really cool text video model. The anime diff lightning model generates stylized videos based on a prompt and can be ran 100% locally. All we need to do is run the sample code that comes with it. You may need to install a couple more packages such as diffusers, transformers, and or accelerate. I made a couple of changes such as increasing the number of steps, choosing a more stylized base model, and enabling CPU offloading so we're not limited by our VRAM. I also set the prompt to a rabbit programming at a computer. The model had to be downloaded but that's only a one time thing. Afterwards, you're free to generate as many animations as you want. The video generation is extremely quick and in VS code, we can easily see the output jiff afterwards. There are so many cool open source models on Hugging Face that you can simply download and run locally, so it's definitely worth your time to do a little exploring. For stable diffusion specifically, there are also other web UIs like comfy UI or automatic 1111s. So definitely consider checking them out if stable diffusion is something that you're particularly interested in. Now we're going to be installing another tool that's not directly related to machine learning but is still used very extensively by the community. Docker will allow us to run separate containers that each have their own configuration and dependencies such as PyTorch, TensorFlow, or CUDA. Docker has an install script provided on their website. I prefer the convenience script because we won't be serving any real production traffic. I like to set it up such that I don't have to be rude to use Docker commands which can be done by following these instructions. And now after running the Hello World Docker container, we can see that we get the success message. Everything is working as intended. Next, we have to enable our Docker containers to use our GPU. This can be achieved with the Nvidia container toolkit. We follow those steps on Nvidia's website and run those commands. Then we can run this final command to ensure that our containers are set up properly with our GPU. Ta-da! This output from Nvidia SMI was ran inside of a Docker container. So now that we have the ability to run any Docker container on our server, even the ones that need a GPU, I wanted to show you guys a really cool review. This library provides several different environments in which you can train virtual robots. The idea is that rather than wasting time and resources to train a robot in real life from scratch, you can virtually train thousands of instances of it in parallel and then transfer the final model to the real world robot. Obviously the challenge is making sure that the virtual environment is as close to the real world as possible. But that's where Isaac Sim comes in. We're going to be using Nvidia's Isaac Sim Toolkit to create a virtual environment. We're going to be using Nvidia's Isaac Sim Toolkit, which is built under their omniverse platform. For this section specifically, I won't dive into the exact steps I went through. I'll just leave the instructions in the description. After starting an interactive bash session in the Isaac Sim container, I clone the Isaac Jim environment repo in the container to run a couple training jobs, just so we can check them out. The really cool part is that we can remote into this container and see what's going on as if we're working directly on our desktop. I'm a gaming computer after downloading the omniverse launcher from which I can then install the omniverse streaming client. After starting it, I can give it the IP address of my server. And now we can see the Isaac Sim interface and how all of our robots are training in parallel. There are some really cool environments that have everything from little ants to robotic hands practicing their dexterity, factory arms learning how to screw a nut onto a bolt and even toy quad copters exploring how to fly. The really cool part is that when I went to Nvidia GTC last month, I actually attended a presentation by some Nvidia researchers using Isaac Jim to train the robotic hand to rotate a cube to match a given orientation. If you'd like to learn more about it, definitely check out my previous video. So far, we've been able to install and run some really cool programs that the ML community has put together for us. None of them relied on TensorFlow though. The other popular deep learning framework. But I wanted to quickly take a couple minutes and walk you guys through how to install and set up the GPU accelerated version. We first need the CUDA toolkit, which is a development environment that also contains additional GPU accelerated libraries. It contains libraries that TensorFlow uses to be able to run on the GPU. Also, it contains the full CUDA runtime and the NBCC compiler, allowing you to write compile run and debug your own CUDA code that runs directly on the GPU. The reason we didn't need to install this for PyTorch is because PyTorch already ships with the necessary CUDA binaries. To know what version of the CUDA toolkit we need, we need to go to the TensorFlow website's download section. If you scroll down, you'll see that the requirements are listed here. Thankfully, we already have the drivers installed and ready to go. At the time of writing this video, it seems like we need version 11.8 for the CUDA toolkit. We also need to download CUDA and separately, which we'll do afterwards. I recommend always using the run file to install the CUDA toolkit. It allows you to better be able to manage multiple versions of CUDA on the same machine and doesn't override your graphics driver, which can prevent further downstream issues. After you run the commands to install the CUDA toolkit, making sure that you de-select the driver option, we need to add the location of the CUDA binaries to your path so our OS knows where to find the programs that ship with the toolkit. Because I want my path additions to persist beyond the current shell session, I'll add it to my .zshrc, replace .zshrc with .bashrc if you're using bash. Now, after I refresh my terminal and type in NVCC-version, we can see that the CUDA compiler successfully displays a version number. Installing CUDA and N is also pretty straightforward. Again, I just run the commands listed on the CUDA and N website and run the final command that's meant for a CUDA version 11. Now that all of the prerequisites are met, installing TensorFlow is literally just one more command. I create a new and a cond environment named TensorFlow and set it to Python 3.11. And then I run pip install TensorFlow and CUDA command with the N and CUDA and brackets. To ensure that everything works as intended, we can run the following commands. They can be found on the TensorFlow website for you to copy and paste. The first is to check if TensorFlow is working and the second one is to ensure that TensorFlow can see our GPU. The nice thing is that if you don't want to set up the entire CUDA toolkit, you can simply use TensorFlow in a Docker container. All you have to do is run this command to download a Docker image with TensorFlow and the CUDA runtime already set up. After the download, it will run the same TensorFlow command to print the name of the GPU, no CUDA toolkit necessary. Well, there you have it guys. This is the machine that I'll be using for all my future machine learning projects that I'll be making videos on. I have so many cool things planned for you guys that I just cannot wait to share. I think my main takeaway from this project was just how important reading the documentation is whenever you're trying out new tools or software. And when that doesn't work, you may have to get your hands a little dirty and persistently scour the internet for a solution. But if you enjoyed this video or have any specific questions about anything that we set up today, please let me know down in the comments below. But until next time, see you guys.