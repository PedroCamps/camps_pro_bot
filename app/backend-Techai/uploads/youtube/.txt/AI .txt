 Hello James, can you explain to the audience what NVIDIA NIM is? NVIDIA NIM is a suite of inference microservices that simplifies the deployment of AI models at scale, providing optimized containers for popular AI models and enabling enterprises to maximize their infrastructure investments. Okay, that's pretty cool. Can you also explain what NVIDIA NIM agent Blueprints is? NVIDIA NIM agent Blueprints are pre-configured, reusable templates that streamline the creation of NIM agents, allowing developers to quickly deploy and manage AI models in production environments. Okay, that sounds pretty cool. I'm going to try that out. So thank you for your help and have a great day. You're welcome. I'm glad I could help and I hope you have a great experience with NVIDIA NIM. Okay, so what we just heard from James here today, we are taking a look at what are NVIDIA NIM agent Blueprints. So you can see here these agent Blueprints are reference workflows for a canonical generative AI use cases. So enterprises can build and operationalize custom AI applications, creating data-driven AI flyways using NIM agent Blueprints along with NVIDIA NIM microservices that we have played around with on this channel before. So I thought we're going to look at the three different Blueprints we have available now. So we had a look at the digital human. We have this rag set up with PDF data extraction and we have this January virtual screening for drug discovery. So I thought we can dive into these three different Blueprints. They take a quick look at them, try them out and see how they work. So yeah, let's just get going. So if you want to try this out now, you can just follow the link in the description. We can go to buildnvidia.com right and we will find this explore NIM agent Blueprints. View all agent Blueprints right and here we can see we have three different ones we can try out now. So if we take a look at this multimodal PDF extraction, we kind of have some files we can choose from here. So these are pre uploaded from NVIDIA that we can try out right and you can see we have the NIM here. So we can look at the setup. So we are using like Google deploy and video re-ranked mistrels. This is like a rag ranking system. We have the embedding model. So you can kind of dive into what we are using on the back end here right. So let me show you how this works now. So let's say we go to public earnings, 370 files and you can see we can click on extract data sets. This is lowing up our data set. It's ready to test right. So we can ask any question here. So let me just come up with some question. Okay, so let's do how much money did Apple make on iPhone sales in 2023. So you can see this should be pretty quick. Okay, so you can see Apple made two, is that 200 billion? I think so or two on iPhone sales in 2023. And what's nice here now we can see we have this resources here. We can look up right. So I think that's pretty neat. I'm going to zoom in on this but you can see here the revenue right is 200 million. So that's up from 2022. So that was pretty quick. Let's do another quick question here. So let's do it in the last three years. What years sold most max descending order. So let's see if we can get our response on that. 2022 had the highest followed by 21 then 23. So we can check that. Okay, so I guess you barely can see it. So 22 was 40 billion, 21 was 35 and 23 was 29 billion. So yeah, pretty good. Now let's let move on and just try another blueprint. Shall we? Okay, so let's go into this generative virtual screening pipeline. So this is way over my head. But let's just try it now so you can see we have alpha fold two. That's pretty cool. That is the big Google folded proteins model. I'm not going to go into any specific here, but let's just try to change up some inputs here and target protein. That's too hard for me. So let's just try to click here, generate molecules. Because I think this has a pretty cool interface, right? So let's just let this cook for a while and I'll take you back when we can see this. Okay, so now you can see we kind of how our molecule here we can zoom in about on it, right? And we can kind of look in all directions. So this must be pretty cool if you are working on this, right? Look at this. I don't know, even what this is. It's some kind of protein, I guess. Yeah, I really think the interface here is pretty cool. And we can kind of look at these different molecules, right? We can zoom in on them. There are different stuff here we can look at. But again, way over my head, amino acid sequence. Yeah, this is nothing I can go into that. But we have the option here, maybe professionals can log into this agent blueprint, get up right away and start using this. So yeah, let's move on. I want to take a bit more close look at the human digital human that is kind of my favorite so far. So I guess you can also me use this on the intro, but you can see we have some different options here. So yeah, turn off this dark reader. So we can deselect or select and videos, productly market knowledge base. So I use that in the intro, right? Why if we deselect this? And then I think we can just talk to James without using that, so let's just click to speak and just do like a short conversation. And then I'm going to show you kind of the gift. We have access to with this to Nvidia and image blueprints and kind of move on to that. So yeah, let's talk to James. Hello, James. So I'm trying to play a world of workraft classic on my 4090. Is that going to work out good for me? The G4's RTX 4090 is a powerful GPU and it should provide a great gaming experience for world of workraft classic with high frame rates and detailed graphics. Okay, that sounds good. So I'm trying also to tank Ragnaros in molten core. What resistance should I prioritize? For tanking Ragnaros in molten core, I'd recommend prioritizing fire assistance as he deals significant fire damage. Okay, I'll go ahead and do that. What kind of weapon is the best drop from Mr Ragnaros? The best weapon drop from Mr. Bigglesworth is likely the sulfur as the unopened high damage to handed mace. Okay. Mr. Bigglesworth, who is that? Okay, I never don't know. Mr. Bigglesworth is a boss in molten core, a raid instance in world of workraft classic. Okay, I haven't done him, but thank you anyway. Have a great day and we speak soon, right? You're welcome. Have a great day and I'll look forward to speaking with you soon. Bye. Okay, so Mr. Bigglesworth, yeah, that's a cat in the axe ramas. I totally forgot about that. That was pretty funny. He got the fire resistance correct, right? So that was pretty cool. I think that's this is pretty cool. I like this and I have a, I'm going to share like a few ideas on how around this agent blueprint that I might try out in the future, right? But let's dive over to the Github repo. I'm going to leave a link in the description for this because here you also have access to all the blue prints here. So you can see we have this repo. So we can go into the digital human repo here. We have the architecture if you want to take a look at that. And here we're going to have a get started pre-requisites. All you have to do, we can also go back here. Let's say the digital human, we can click on this blueprint card. And here we can kind of see the system requirements. So this is a compute heavy pipeline, two times A100. Yeah, so you kind of need like a some compute to actually run this digital human, right? But we have a good installation here. So we're going to run like a Docker, GPA, we're going to run this GPU drivers in file for three. We have the install the container to cool kits. You can follow along here. And if you go back here, you can see you can access all of these different Github repos if you want to set this up for yourself. Yeah, so you can see here again, this is the multi model one. So I guess this has less requirements, right? So a lot to play around with here, we want to check it out. So as we probably understand by now, my favorite blueprint is the digital human. So I had some different ideas around here that maybe we can use in the future, right? So here are kind of the names we are using for this now. We have the audio to face. That is the video we see from James with the lip syncing, right? And the facial expressions. And we run kind of llama 3 8B in the backend. And we also have this that kind of transfers our audio to text for llama 8B, right? But let's say we wanted to expand on this digital human. So I thought about was it would be pretty cool if we can add like this VLM. So that's vision language models like let's say we could swap out llama B and we can also introduce like a new name here that can take vision in right? So we can use the camera. And let's say I can use the camera here to kind of show our digital human something, right? Or upload some kind of files. I thought I could be pretty cool and if you kind of see my facial expression. So I'm hoping for something like that in the future and that kind of will bring the realism to another level. And I think like in games, let's say the human or like the digital human can kind of see your facial expression too. And kind of pick up on some emotions in your face. I think that could be pretty cool. So yeah, if you want to try this out, just follow the link in the description. It will take you over to here and you can kind of I would really recommend start to try talking with this digital human. It's pretty cool if you ask me and also check out the other page and blueprint here. Head over to get up if you want to explore even more into the code and stuff. So yeah, pretty cool if you ask me looking forward to see where the next step of this NVIDIA name microservices is going to be. So if you enjoy this, give this video a like. Thank you for tuning in. Have a great day and we speak soon.