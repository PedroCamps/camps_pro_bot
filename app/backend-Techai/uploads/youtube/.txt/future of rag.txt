 Retrieval Augmented Generation has been the go-to way for feeding external knowledge into LLM since the dawn of generative AI. It is the classic method for turning an LLM into an expert at something that you care about, like your favorite agent framework or your e-commerce store. But if you've actually tried to implement Ragn before, you definitely know it's common pitfalls. The things that make you want to pull your hair out, like the wrong text being returned from the search, and the LLM completely ignoring the extra context that you give it. The kind of things that make it so that even though Ragn seems logically sound to you, in theory, it just completely falls apart in practice. And you are certainly not alone if you feel this way. That's why there's so much research in the industry right now for how to do Ragn better. And there are a lot of strategies out there, like re-ranking, query expansion, rank normalization, things that I'll cover in other videos on my channel. But out of all of the strategies that I've researched and implemented myself, Agentic Ragn is the most obvious, it works the best, and it's exactly what I'm going to show you how to do in this video. I'll show you how to take standard Ragn and then turn it into an agentic approach that actually delivers. So you don't feel like throwing your computer out the window because nothing is working for your agent. In the last video, I showed you how to use crawl for AI, an open source LLM friendly web crawler to scrape entire websites super fast for Ragn. And we use the Panantic AI docs as an example. Now we're going to take this much, much further because we are going to ingest all of the documentation into a database for Ragn using a super base. Then we'll use Panantic AI to create this agentic Ragn agent leveraging our knowledge base. And then finally, we'll use Streamlit to create a UI to interact with our agent very seamlessly. Through this, I'll walk through what agentic Ragn is and why it solves the common pain points that we often see with Ragn. All at the same time, building a live example for us to see this in action. So buckle up because I am shoving a lot of value into this video for you. I'll even talk about things like Quadrant versus Super Base for Ragn. How you can expand the heck out of this setup and also how you can try this agent for yourself right now with no setup. So with that, let's dive right in. Now the big question that I have to answer to really set the stage for the rest of this video, or I do a deep dive into an agentic Ragn solution, is what is agentic Ragn? And there are a lot of websites and articles and diagrams out there on the internet explaining it. And the best one that I have found is this article on the Weve8 website, which by the way, Weve8 is a vector database platform. And I'm not going to read through this entire article here, but really it's these first two diagrams that I really, really appreciate to help us understand what agentic Ragn is and why it is useful. Because this first diagram, which I'll zoom in on here, is what is basic Ragn? It explains this very clearly. Essentially, you have a knowledge base made up of a bunch of documents. You split them into chunks so that we have bite-sized information to efficiently give to the LLM. And then we turn all of these chunks into vectors. They're mathematical representation using an embedding model. And then we store that all in a vector database. So essentially every chunk is turned into a large string of numbers and a vector that represents all of that information. And then when a query comes in from the user, when you're interacting with the agent, that query is then turned into a vector representation just like the chunks were using the same embedding model. And then using some mathematics under the hood, some vector math, the query is essentially matched to the most relevant documents that are then given as context to the LLM. So essentially the prompt is expanded to include whatever relevant context was retrieved from the vector database knowledge base. And so you have something like, here's the user question. And then here's all of the relevant context for you to answer it. That's essentially what the prompt then looks like for the LLM, which then gives the final response using all that information to augment its response. That's why it's retrieval augmented generation. And you can kind of see the downside to this. It's what's called what we call a one shot here where the vector database is retrieved from. We give that extra context, but then the agent can't actually reason about what it was given. It can't decide, oh, this wasn't enough context. So I need to look again or search in a new way. There's no opportunity for the agent to actually improve upon whatever it got for that additional context. And so that is what a genetic rag does. Let me zoom in a little bit on this diagram here. This is a genetic rag, the second diagram in this we be at article here that I absolutely love. Basically, instead of just having rag be a one shot, here's some context from the vector database. We actually create rag as tools for the agent to interact with, which unlocks things like even being able to search through different vector databases. So an agent based on the user question can reason about where it will actually go to find the knowledge. And you can have other tools that can search the knowledge in other ways. And so you're giving the agent the ability to intelligently explore the data and not just work with what it's given in the first shot. And that is the power of a genetic rag. And I don't like these tool examples in particular because these aren't really related to the knowledge base. But you can very much imagine other tools where maybe for the patented AI documentation, you would give it the ability to see the URLs for the patented AI docs and actually pick to search in a specific page based on the title of the documentation page. And that's actually something that we are going to be implementing. And there's one other diagram that I really like as well, which is this one. I think this is very similar, but it explains it very nicely as well because essentially you have your large language model layer. Here's your AI agent. Here are the tools that it has access to. And the retrieval functions that actually go to things like the vector database. That's just one of the options. You have all these other tool functions and action functions that can work with the knowledge base in other ways, really allowing the AI agent to reason about how it is searching for the answer for the user. And that's what makes it so much more powerful. And hopefully this example that we're going to go through what we'll be building in this video will make this whole idea of a genetic rag super crystal clear for you. All right. So there are a few moving parts for building our agent rag solutions. So I've created this simple and beautiful diagram here with the help of Claude to walk you through what we're going to be doing in this video together. Because I'm breaking it down step by step to make this entire process very digestible. But then still by the end of this video, we're going to have an awesome solution here. And so the first thing that I'm going to be doing with the rag website crawler is essentially taking what we did in the last video with crawl for AI, scraping the content of a bunch of websites. And now actually putting it into a database so that we have our rag knowledge base. And I'll do this before we're even doing anything with the database set up because I want to get into the code right away, dive into the meat of it. And so then once we have this script set up, then we'll go over to super base, do the very simple set up there. I'll walk you through everything and then we'll run our script and then boom after these first two steps, we have our knowledge base ready to go for an AI agent. And so naturally the next step is using a pedantic AI to create the foundation of our AI agent. And we're actually going to start with just basic rag because I want to give you clear examples of when basic rag is enough versus when we need our agentic rag approach. And so we'll take these negative examples of when rag doesn't work and test it again once we move on to actually implementing agentic rag. So there's a couple new tools that will introduce here to make it a very robust solution. And after that, I'll talk briefly about the streamlit UI and show you how I built that so that we have our interface and the entire time as we're doing our testing, I'll be using the streamlit interface and then just referencing it at the end showing the code off a little bit for that. So that is our entire process breaking it down step by step. Let's dive right into it. First things first, if you want to try the exact agent that we're building in this video for yourself right now with no setup, you can do that. All you have to do is head on over to the Automated or Live Asian studio, sign in, you'll get some free tokens. And I have the exact agent recovering this video hosted right here. So it's a really cool way to actually go and throw some of your questions at it, explore the pedantic AI documentation and test out what we are building right here. And it's really awesome because there's even some example questions for you to get started here. Everything, like I have not changed the code at all, everything is the same. So yeah, in this case here, I'm asking it for an example agent, specifically calling out the weather agent that I know is in the pedantic AI documentation and boom, look at that. We've got the full code example right from the documentation. So yeah, this agent is awesome. It saves so much time having to scrounge the documentation because the agent is doing it for you with rag. Super neat. All right, let's get into coding our agentic rag solution. So everything that we are going to create together here, I will have any GitHub repository that I'll have linked to in the description. That is all the code that we are looking at right here. So first of all to ingest our documentation into a knowledge base, we've got this script right here. Everything that we currently see is just what we already did in the last video with crawl for AI. So we'll be expanding this to actually work with a database. Then we've got the sequel. I'll show how to run all of this later to set up our database. This is our pedantic AI agent that we will build together from scratch. And then finally, the stream interface, I'll go over the code for this little bit later as well. And then we've got a nice read me in the repository to walk you through everything and how to set this up yourself as well. And so going back to that diagram that I showed earlier, I just showed you the code for every single step that we've got here. So I'm going to break it up step by step. And I'm going to make this as simple as I possibly can. So in the last video on crawl for AI, we covered this repo, went through their documentation and some of their examples on multi URL crawling to get it so that we could give all of the documentation pages for pedantic AI. And actually get the contents of every single one of them super super quickly by doing parallel scraping on all of these pages. And so going back to the code that I've got right here, that is what this script is doing so far. So again, this is mostly just what I coded up in the last video on my channel or recall in parallel. All of these URLs that we fetch by getting all of the pedantic AI documentation URLs from the site map. So this page right here lays out the entire directory of the pedantic AI documentation. So we just process that XML to get all of our URLs and then we crawl them all. That's it. And so now in this function, we have a new step right here. So after we crawl the page and get the markdown in the result here, now we actually want to add it to our knowledge base. So I'm zooming in a little bit here, focusing in on this function right here. So this is not implemented yet in the script. That is what we're going to add. So after we get the content, we now want to process it as in create the chunks, get it ready to insert into our knowledge base and also define all the metadata and things like that that we will get into. So let's go ahead and do that now. So I'm creating a new function here called process and store document. That's what's called in our crawl function. And it's job is to orchestrate everything that we need to get the information ready for the knowledge base. So first of all, we want to chunk our text. And the reason for that is when we store our information in the database for LLM knowledge, we don't want to store a massive page just in one record for the LLM to retrieve. Because if we have a documentation page, it's like 50,000 words. And we give that all to an LLM at the same time that is going to overwhelm the prompt and just make the LLM super confused. So we want to split up longer pages into smaller chunks so that the LLM can retrieve the specific knowledge it needs from a piece of documentation without pulling in the entire thing. So we chunk the text and we'll define this function later. And then in parallel, we want to process all of these chunks. So at the exact same time, we're going to do what it takes to actually turn all of these chunks into embeddings that we can put into our vector database. In this case, using super base and we'll do the setup for that in a little bit. And then the last thing that we need to do is store the chunks. So we split them up, we process them, and then we store them super simple three step process. And so now all we have to do is define all of these different functions for chunking processing and storing. Let's go ahead and do that right now. So the first function that we want to define is the one to chunk our text. And the chunking can get a little complicated because there is a lot that goes into taking a single page and breaking it up into smaller bytes for the large language model while still respecting things like code blocks and paragraphs. Because think about it, if we have to split a document at any given point, we don't want to split it in the middle of a paragraph or the middle of a sentence or a code example. We want it to preserve that so that one chunk has all of the information that it needs. You don't have to look at the start of a next chunk to finish the sentence or the paragraph or something like that. And so because this is a little bit more complicated, I actually used AI quite heavily to help me create this function. And you can use that as a rule of thumb like anything that's a little bit more complex that I code here, I definitely used AI to help me with and you can do the exact same thing. So I'm not going to explain this function in a ton of detail, but just know that I'm taking a single document, turning it into bite size chunks and respecting code blocks and paragraphs when I do it. And so the first thing I'm going to do is just loop through all the text in the current chunk, basically just defining the boundaries for it. And so if we're at the end of the text, we're just going to take what's left because we're at the end, we're not going to split another time. Otherwise, we're going to try to find a code block. So because crawl for AI is returning markdown to us, I know that this is the syntax represent the start and end of a code block in markdown. And so I try to find it and actually adjust the end to include the entire code block if I find one there. And so that's how I respect code blocks. And then I do the same thing for paragraphs and sentences as well. And so that way at the very end of a chunk, we are not split in the middle of a paragraph or a sentence or something like that. And then we just extract the chunk and clean it up. And so now at this point, we have a final chunk that we can add on to our list of chunks. And so when you combine all of these chunks together in this list, that creates that full document. So that's how we have basically just an array. Like if I go up to the top here, this function returns a list of all the chunks that together make up the entire document that we just split up. And then we'll just return all the chunks at the end. So a little bit more complex, but the idea is very simple. You can very much see like we get the text and we're given back a list of chunks so that we can insert them one at a time in the knowledge base so that the LLM doesn't get super confused pulling in the entire document at once when really it was just looking for a small portion of the knowledge in that page. So that is our function to chunk our text. And so at that point, we've gotten past this right here in our main function, do process and store every page. So now the next thing we have to do is process each chunk and you'll see in a little bit what that actually entails here. So first we're going to start by defining our data class that actually defines what information we get in a chunk when it is fully processed. So we have the URL for the pedantic AI documentation page. We have the chunk number, like maybe if we split the document into 10 different chunks, this is chunk number 2 out of 10. And then we're going to give a title and a summary for every single chunk. And this is going to get into a genetic rag a little bit. But if we have this extra context about what this chunk represents, that's going to help the agent reason about when to use this specific piece of knowledge. So more on that later, then we have the actual content of the chunk, some metadata, we'll get into that as well. And then finally we have our embedding. This is what actually allows for reg where we have kind of the vector database part of what we're storing here where we can do that retrieval with vector mathematics here. So that's our process chunk. And that's what we're going to use in this function right here to process our chunks. So the job of this function is to take the chunk in the raw string format and turn it into this class right here with all that information I just went over. So the first thing that we want to do is actually use a large language model to extract a title and summary from this chunk. So we'll define this function in a bit. I just want to lay out this entire process first. Same thing with this next function. We want to get the embedding. So this is the actual vector representation of the chunk content, which is what we're going to use for reg. And then finally we will create the metadata for our chunk. And so this includes things like the source. And so really metadata is just additional information that you attach to the record so you can do specific filtering. So for example, I say that the source is pedantic AI docs. And the reason I do that is this actually makes it so I can use one database table for my knowledge base for many agents. Because if I have a pedantic AI agent and then I have like a crawl for AI agent and then a Lang chain agent, I can have them all use the same knowledge base. But whenever I query the knowledge base for that specific agent, I would just say, give me the results from your search. But only looking at the records where the source is pedantic AI docs. And so I can segregate my knowledge in a single table for multiple agents using metadata. And I can also query based on time, for example, if I only want to search based on the records in the last day that we're ingested in the last day, I can do that with this crawl that metadata. So that's the importance of metadata. It's super key to have good metadata because that gives you a ton of options that it locks so much as far as additional filtering that you can do. And that's a whole advanced concept in it up itself. But I just wanted to include that here just to show that you can take filtering very far with reg. And then finally, we're just going to return our process chunk. So we have the URL at this point that was given into the function. We have our chunk number that was also given in. And then we have the title and summary that we extracted with a large language model, the content, the metadata, and then the embedding that we created. And so now we can define the function to actually get the title and summary. And so there's a nice little system prompt that I added here to instruct the large language model, which I'm going to be using, GBT 4.0 mini. But we instructed on how to extract the title and summary. And we're looking for a specific JSON object. So that way what we get back can be referenced in this way where we want the title that is extracted. And then the summary that is extracted. And so it's very simple. We're just going to make a call with our open AI client that we're giving here where we are using GBT 4.0 mini or whatever you define in your LLM model environment variables. You can tweak this as well. And then we give it the system prompt and then also the chunk, just the first 1000 characters. That's all it needs to then create a title and a summary. And we return a JSON object as our response format. And that is what we return. So we just load it as JSON, whatever we get back from the LLM. And that is going to again include our title and our summary. Very, very simple. And then if we encounter any errors, we'll just return that. We'll tell that to our terminal there. So we know that there was some issue trying to actually process this specific chunk. And then next we want to define a function to get the embedding. And so this again is what we're going to be using for the actual basic rag when we want to do that search with math vector mathematics. And so we're going to be using again the open AI client, just using their text embedding three small model, giving the text from the chunk and then just returning the embedding super simple function. Open AI makes it so easy to get our embeddings very, very easily. So that is that. And then we'll just handle any errors as well, just like we did before. And if we do get an error, we're just going to return a zero vector here. Just so that the process can continue. But we at least have some value for the embedding there. So that is it for processing a chunk. Now we can get on to our very last step here, which is actually inserting chunks. We reference this function in our main procedure here to process and store our documents that is inserting a chunk. This is where we actually put it into super base. Now everything that we are inserting here is not just for basic rag. You'll start to see once we get into a gentick rag, how some of the things that I'm inserting like the URL and the title in the summary can be used to understand the knowledge and more ways than just basic rag with the embedding. So we have the embedding for basic rag, but also this other information so that we can explore the knowledge in different ways. And so we'll get into that little bit. First, let's finish off this function here to actually insert this data into super base. So first we're going to create an object here, which has all the data that we want to insert just based on the chunk here. And then we're going to insert into the site pages table. So we'll create this in a little bit here. This is the sequel to actually create this in super base, but we're inserting this chunk into the table. Very, very simple. And we'll just handle any errors. And so that is everything. We have now completed this script and there's a lot that went into this actually creating the AI agent itself is going to be much simpler. But now we can get on to setting up our database tables and then running the script to create our knowledge base. So the sponsor of today's video is an open source platform called GPU stack and I'm super excited to bring this to you today because it is free to get started and solves a huge problem in the AI space, which is scaling local AI. And trust me when you really start to build an application around local AI, whether you have your machines hosted in the cloud or you built them yourself, you're going to really need to think about how you manage your different GPUs and all of the AI inference going on on your machines. And that is what GPU stack can help you with because it is an open source GPU cluster manager and it is phenomenal. It supports any hardware that you could be running on. You can dream of any large language model that's going to be accessible and it supports single node multi GPU setups as well as multi node setups. Now the obvious question is why use a GPU cluster manager and the easy answer is if you have tried to use more than just one GPU for your local AI stack, you know how important and difficult it is to monitor your GPU usage and also efficiently dish out the inference to your different resources. And that is what GPU stack makes very easy. It is so easy to monitor your GPUs in the platform, set up your clusters, add GPUs into them and just manage your entire local AI stack. They have a open web UI like interface, which is very, very clean with a huge focus on doing all that monitoring and managing of your clusters. And also you can set up your stacks as open AI compatible endpoints so you can really hook in your clusters into any system you could dream of. So I will have a link in the description to GPU stack. I would highly recommend checking it out because as soon as you use local AI for an agent or an AI application for anything but just yourself, you are going to need something like GPU stack to monitor all your resources as you are scaling your application. So we have just finished this step right here. Now it is on to the super base setup and then we can run the script to create our knowledge base. And luckily what we just went over is actually the longest and most complicated part of this entire process. Once we actually have our knowledge base setup, creating the agent interact with it is actually pretty simple. And so let's head on over to super base right here. I'll assume that you already have a project created in super base. The rest of it, I'll walk you through everything. So this is the home page for a super base project once you have it set up. All you're going to want to do to get things started is go on over to the SQL editor tab right here. So you'll click on this and then you'll have this blank slate right here where you can paste in all of the code in the SQL script that I have in the repository. So going back to our code here, that is this. So you're just going to want to copy this and I have this setup specifically for super base, including a role level security policies as well. And so then with this, you'll just paste it in right here and then go ahead and click on run. And so this is my first time setting up for the super base project. So I am doing it completely from scratch with you right here. And then once you have this run, you can just go to the table editor here. And then boom, there we go. You've got our site pages table with nothing in it right now. So now it is time to actually set up our environment variables, including our super base URL and service key. So we'll insert that all in and then run our script to get everything in this table here. And so first of all, it's actually get our super base credentials. You want to go down to the project settings in the bottom left here and then click on API. What you're going to want to do is copy your super base URL and then also your service role secret. These are the two environment variables that we need to give our script so that it can actually insert the knowledge into the table, the site pages table that we just created. So copy those two things and then head on over back to the code and open up your dot e and v dot example file. So you're going to want to take this, rename it to dot e and v like I did here. I'm not going to show this file because that actually has my credentials. But you're going to want to take the super base URL and service key that you just copied from the super base UI and then paste those here. You're also going to want to get your open AI API key, which I have instructions here on how to get it just right here in the dot e and v dot example and then define the large language model that you want to use. So in my case, I'm using GBT 40 mini. This is exactly the ID that you would just set it to right here to reference it. That's it. We now have all the credentials set up. And so we can go ahead and run this script that we just created and create our knowledge base over in the terminal now. Let's go ahead and run this. So I have everything installed, including my Python libraries already. I've got instructions on the re me for how to do that with the requirements dot text file. So I'm in the current directory with that script that we just created. And now all I have to do is run the command Python and then the name of this script crawl, pedantic AI docs. And this is actually going to be quite chatty as it processes all of the chunks and sets them all in the knowledge base. But it's kind of cool to see this all roll through and it's running in parallel in batches. So I think 10 pages right now. So I also batches of 10 chunks at a time. But it's cool to see it make all the calls open AI to get the titles and the embeddings and the summaries and the calls to super base to insert in the knowledge base. It's pretty neat. So I'm going to pause and come back once this is done and just like 10 to 20 seconds. All right, there we go. We are at the end of our script and we have everything in our knowledge base now. Also, if you run into any rate limit issues with open AI or anything like that, you can also just change the batch size and also add in some delays with the time Python library. So play around with that. If you have to, I didn't run any of that myself. But I know some people did from my last video when I did this with crawl for AI. So let me go out of this for now. And then let's go over into our super base because we're going to check out all of the knowledge that we just added in. So I'll go back over to the table editor here, go to site pages. It says it's empty right now. There we go. Now refresh. We got 245 records. So out of the 42 pages for the panatic AI documentation that we scraped from this right here. We turned it into 245 chunks and we've got the URL, the chunk number, the title and summary that was created with GBT for all many for every single chunk, the content metadata and then the embedding as well. That's the massive vector representation that will actually use for basic reg. And so with that, we can now move on to our next step here, not this diagram, this one, which is creating the AI agent for basic regs. Let's go over and do that now. So this is super meta, but we are building a pedantic AI agent to be the pedantic AI expert. And so if you want help building agents like one about to build right here, you can literally refer to this agent to help you code your agents and also refer back to the pedantic AI documentation. And so yeah, let's dive right into creating this agent here. And so first of all, we're going to start with just a basic reg example, then we'll extend this exact same agent to do a lot of really cool stuff with agentic reg. So the first thing we're going to import all the Python libraries that we need and then do some of that initial setup, like loading our environment variables, including the specific model that we want to use from open AI, configuring log fire. This is the login monitoring for pedantic AI, which I'm just going to disable for now. I'll set this up maybe in another video on my channel and showcase that. And then we want to define the dependencies for our pedantic AI agent. So the way that you typically set up your agents with pedantic AI is in three parts. You have your dependencies, which is things like the super base client and the open AI client that your agent needs access to to use in the tools. That's number one. Then you set up the actual agent itself. That's number two. And then number three, you define the tools for your agents. So we're going to walk through all three of those. This is step number one with the dependencies. And then we're going to create our system prompt here. And this is a pretty rough draft of a system prompt. I haven't said too much in this here, starting really, really basic, just describing what kind of tools it has, what the role is of this agent, even kind of pointing to some things that will have for a gentic reg and a little bit here. But you can just ignore that for now. Anyway, that's our system prompt. And then we'll create our actual agent. So step number two, we define the agent using the agent class from pedantic AI. We specify our model that we defined above with open AI. We give it our system prompt. We tell it the kind of dependencies that the agent will have access to. And then also there's retry logic for your LLM calls baked into pedantic AI as well, which is super neat. So we've already done step one and two with dependencies and the agent. And now we can define our first tool for the agent. And that is going to be the tool for basic reg. And so before we even define that, I'm just going to throw in this function right here to get the embedding for the user's question. So we get the embedding for the user question and then use that to check the knowledge base with reg. So this is basically just the same function that we created in this other script right here. So I'm not going to go over that again. Now we can define our tool to retrieve relevant documentation. This is basic reg in its finest. So what we're going to start out with is a dox string here because in pedantic AI, this dox string that you have at the start of a function is how you tell the agent when and how to use this tool. That gives it all the context it needs like the arguments and the purpose of the function. And the way that we turn this function into a tool for the agent is with this what's called a decorator right here. So we put this at the start of our function with the at symbol and in the name of our agent dot tool that immediately turns it into a tool that's accessible for our pedantic AI agent. And we can continue on with the code for it here. So first of all, we're going to get the embedding for the user's query. And we're also passing in that open AI client. So that our agent has access to open AI to do this. And then we are going to run this procedure here. This is what actually does reg within our site pages table. So if I go back over to super base here. And I go to the script that we ran. I'll just scroll down right here. We have this function that we created called a match site pages. This is all the logic that happens in the database where we can give it a query, the embedding for a query. And it'll find the records in the site pages table based on those embeddings that are the most similar. So what we get back from running this is going to be the top and then we default to 10. So the top 10 by default records that match the user's question the most. And so back over to VS code here. Actually, I'm specifying the match count as five right here. So I'm getting the top five most relevant document chunks. And I'm also doing that metadata filter. So I'm only going to be pulling knowledge from this table here where the source is patented AI docs. And that way if I have other agents that are also storing knowledge in here, I'm not going to overlap and accidentally pull knowledge from those agents as well. Obviously right now the only records in this table are going to have the source of patented AI docs. But this is just a nice little value add for you to show you how you could do something like this with metadata. So pretty neat. So now if there are no results that are returned to us, we're going to tell that to the agent so that it can report back to the user. Otherwise, we're going to format all the chunks. So we're going to take the top five results, put them all together into a nice string and then return that to the large language model. So we're giving it a nice format. So I can understand that, oh, this is chunk number one, this is chunk number two and so on. And then we'll handle any errors as well. So there's any issues when it's retrieving the documents. We'll print that to our terminal and also report that back to the agent. So again, it can go back to the user and say, oh, sorry, I had an issue trying to retrieve from the knowledge base. Obviously, don't want that to happen. But yeah, so that is our first tool. That is very basic rag. And so now we can actually run this agent. We don't have anything that is a genetic rag at this point, but we'll get to that. But first I want to show you when basic rag works and when it does not. Okay, so here we are in the streamlit UI chatting with the padantic AI expert that we just built. I'll cover a streamlit more at the end of this video. But let's dive right into testing out this agent. So I want to give you a good example of when basic rag is enough. And then I'll show you an example where this falls apart and then we'll get into a genetic rag to actually fix that problem. So first, I'll start with a very basic question. What are the supported models? And so this will do that retrieval. It'll perform rag to get me the answer here based on everything that we put in the knowledge base. And there we go. There is a number of App Services's Pantantic AI supporting and it lists a bunch out. And then it even says for a specific model names, here are a list of some of them. So yeah, this is a solid answer. Very much pulled this from the knowledge base. If you were to ask Claude or GPT, some agent that doesn't have access to this documentation, it would completely bomb this question. So that's awesome. It's working pretty well. Now, let's get to an example where just basic rag will not work. Let me actually go to that right here and I go to the examples. They have this weather agent So they've really really awesome example I'd actually recommend checking this out if you want a good example Maybe besides my videos of course on how to build a pedantic AI agent I would reference this right here But if I go back over to the interface here and I ask it give me the weather agent example Let me spell that right example from the documentation It's gonna bomb this I've obviously tested this out a lot before I recorded this exact segment for you I know that it doesn't get this right so it's giving the response here and it looks good so far But you can see that this is pretty bad like It kind of started correct, but it's not defining the dependencies I mean this is just like a tiny script compared to what we actually have here in the weather agent It didn't really give me the example you can see that it obviously pulled some relevant information to get me a half-decent answer, but this still is not acceptable This doesn't actually give me everything that I need to go ahead and then just run this entire agent So that is why we need to move on to creating a gentick reg What we're going to be doing here is actually giving more tools to the agents so that not only can't it perform reg With the embedding column that we have right here But it can actually pull a list of the URLs from the pedantic AI documentation Reason about what pages it might want to look at to get the right information And then actually go and visit those URLs and we can even have it for example use the title in summary to help it figure out Which pages to visit as well and pull the content from so this is where we start to get into Having the agent actually be able to reason about where it needs to look for knowledge instead of just performing Basic regs. Let's get into adding those tools So back over in our code Let's add these tools for a gentick reg now before we even do that I just want to say that you could probably tweak the chunking strategy or the retrieval strategy to make it solid It could successfully answer that whether agent question with just basic reg I don't know for sure But there's so many ways to optimize rag in other ways besides agentic reg But it still just gave a very clear example of how basic reg can fall apart especially when you have to look at bigger pieces of information to give a longer answer Because typically with rag you're just looking at a tiny segment of the entire knowledge base But with a gentick reg we can actually instruct it on how to visit entire pages and pull all of that or parts of the page that it needs to Answer a question and so the first tool that we're going to implement here We're just adding another tool is the one to list documentation pages So this is going to go to the database here again We have a dox string to tell the agent when and how to use this tool But it's essentially just going to go to the site pages and pull all of the URLs And we're doing the metadata filter again just to make sure that we have just the paidantic AI docs and nothing else We're pulling this list of URLs returning an empty list of URLs if nothing was returned Otherwise we're giving these back to the LLM So it now knows which URLs it can reason about potentially visiting to answer the user's question And then we'll also return an empty list if there's any errors as well And I promise at the start of the video I would talk a little bit about Quadrant versus Superbase as well So here's a really quick rundown of when to use either I like using Superbase a lot because it simplifies things This is a very good example I have reg for this function right here And then also structure data that I have to store in a SQL database used in this function All in the same platform because I am using Superbase Quadrant is just a vector database I can only store embeddings And so if I wanted to use Quadrant in my solution I could only use it for basic reg and then when I get into a gentic reg where I'm actually looking at structure data in a table I would still have to use Superbase So I'd actually be using both But with Superbase I can have the reg and the structure data all in one place That's why I generally recommend Superbase I will say though that Quadrant is a lot faster So if I wanted to optimize for speed here I actually would have all my knowledge and embeddings stored in Quadrant have this function reference Quadrant instead of using Superbase with this match site pages function right here And then I would still also use Superbase and have the table for all of the metadata and things like that Like the title and the summary and the URL Everything that I'm referencing in this function So that's a quick little rundown there Now let's get into the second function right here So now that we have the ability for our LLM to get all of the URLs We obviously need to give it the ability to get the contents of a specific page that it wants as well So again another dox ring here telling it when and how to use this function All we're going to do is query the site pages and grab the title content and chunk number from a specific URL And then we're going to return that to the LLM Or maybe just say no content found for the URL if it doesn't find anything And then handle any errors as well Yeah last thing here we'll also do some formatting here just to make the information Nice and easy to understand for the LLM Also combining all the chunks together for that single page So this way the LLM can now read from an entire page And this is going to improve the performance of some of the more complex questions Because now we are implementing a genetic reg Instead of just a basic query using embeddings We're actually giving the agent the ability to look at the URLs and think Oh if they're asking for the weather agent example And there's literally a page where it's like pedantic AI docs slash example slash weather agent Oh I should probably go visit that Pull all the contents from it and then use that to answer the user question Instead of that little bit of knowledge I might retrieve with basic reg That doesn't actually cover the full example And so with that let's go ahead and run this again and test it out So back in the streamlit UI let's go ahead and ask that infamous question That it botched the first time I'll say give me the weather agent example code from the documentation All right so it probably will start with basic reg still Because I instructed to do that in the system prompt But then as a part of the reasoning here I'll instruct it to then go on to actually check the URLs pull the right page And then boom look at this we are definitely getting the full example now And I can even open up my terminal and see That it did start by retrieving relevant documentation Basic reg figured out that didn't give it the right answer So it went on to list the document pages found the right one So it called the tool to get the page content And now that gives us the full response here Even including yeah like literally everything the main function the tools defining the agent This is the full example for us So now we have much better results because the agent can actually reason About the knowledge base and not just do a one shot reg query Last but not least I wanted to cover the streamlit user interface very quickly here Because I set this up as a way to use streamlit to really chat with any Potantic AI agent so it's pretty neat and I don't want to code this from scratch with you right now Because the big focus on the video is everything with agent reg And this is just the way for us to talk to the agent But I wanted to show this off very quickly here And then this is also in the GitHub repository So you can use this to chat with this agent Or hook it into the live agent studio like I've showed in other videos on my channel And you can use this to really work with any Potantic AI agent with streamlit Because essentially I have these functions here That convert the text from the the Potantic AI format to what I can display In streamlit and then I have my main function Where I actually invoke my agent here And I pass in the dependencies that I set up on the streamlit side So I give all of the dependencies like the super base client and the open AI client Into my agent right here Get the response and then I actually stream this out So instead of it being just like boom, here's all the text at once It streams it out in typewriter style Like we like to see with interfaces like chatgbt.com And this even handles the tool messages in the message history So the agent when it calls reg and it gets the context for a specific part of the Potantic AI documentation That is now a part of the message history So it can even reference that later on as well So it doesn't have to pull that page a second time So it's pretty neat Yeah, overall it's a pretty concise function here We got the main function where we set up all the messages in the UI Get the user input and then handle that Whenever the user input something We add it to the conversation history And then we stream the response from the agent Which also adds the AI messages and the tool messages to the message history All in the format that works for Potantic AI So again, I'm not going to dive super, super deep into the script But you have that there for you The main thing that I really wanted to focus on was this agent right here Which by the way, if you want to expand on this There's so many ways to even for this specific example To add a lot of new tools to make this even more robust than an already is For example, just throwing out something really random here You could set up a dedicated knowledge basis Specifically for Potantic AI examples And then you could have a function that is using reg But just for the Potantic AI examples So that way when a user asks for an example You're going to get more accurate results Actually fetching an example Instead of the agent maybe accidentally looking somewhere else in the Potantic AI documentation That doesn't actually have full examples Maybe they're just like short little clips of code And that's not what the user wants When they want a full example And so that kind of goes back to the Webia article Where you can have different knowledge stores And have the agent actually intelligently reason about where it wants to go So that's just one example Or you could even have another area of the Potantic AI documentate Maybe you reference the actual code in the GitHub repository And that could be its own knowledge base And you could have listing and fetching functions For getting specific content in the GitHub repo To actually dive into the code There's so many ways that you could expand Agenetic reg here Like I'm not promising that what I have set up here is going to work perfectly Because in the end there's so many different opportunities To make this better and better That is a wrap for building an agentic reg solution That actually solves a lot of problems we typically see with reg By giving your LLM the ability to reason about where And how to get the right information from your knowledge base You get much more consistent and accurate results And of course this is just one way to level up your rag game There are so many different strategies out there And also ways to build on top of agentic reg And my solution like I covered earlier in this video The main thing that I just really wanted to drive home here Is the importance of giving your agent the ability to intelligently leverage your knowledge base Instead of those one shot queries that you have to get out of your system And I'll be putting out a lot more content on rag and AI agents On my channel going forward as well So stay tuned for that If you appreciated this video and you're looking forward to more things rag and AI agents I would really appreciate a like and a subscribe And with that I will see you in the next video